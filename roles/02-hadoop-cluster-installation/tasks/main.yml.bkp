---
- name: add hadoop user to sudoers
  user: name=hadoop groups=wheel append=yes state=present createhome=yes
  tags:
    - hadoop

- name: Allow wheel group to have passwordless sudo
  lineinfile:
    dest: /etc/sudoers
    state: present
    regexp: "^%wheel"
    line: "%wheel ALL=(ALL) NOPASSWD: ALL"
    validate: "visudo -cf %s"
  tags:
    - hadoop

- name: create hadoop installation directory
  file: path=/opt/hadoop state=directory owner=hadoop group=hadoop recurse=yes
  tags:
    - hadoop

- name: download the hadoop tarball version 3.3.1
  get_url: url="https://dlcdn.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz" dest=/home/hadoop/hadoop-3.3.1.tar.gz
  tags:
    - hadoop

- name: extract hadoop configuration file to the installation directory
  unarchive:
    src: /home/hadoop/hadoop-3.3.1.tar.gz
    dest: /opt/hadoop
    extra_opts: [--strip-components=1]
    remote_src: yes
    owner: hadoop
    group: hadoop
  tags:
    - hadoop
   
- name: edit /etc/bashrc with variables
  blockinfile:
    dest: /etc/bashrc
    block: |
      export JRE_HOME=/usr/lib/jvm/jre
      export JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk
      PATH=$PATH:$JRE_HOME:$JAVA_HOME
  tags:
    - hadoop

- name: edit user profile /home/hadoop/.profile
  blockinfile:
    dest: /home/hadoop/.profile
    block: |
      PATH=/opt/hadoop/bin:/opt/hadoop/sbin:$PATH
    create: yes
  tags:
    - hadoop

- name: edit root profile /root/.profile
  blockinfile:
    dest: /root/.profile
    block: |
      PATH=/opt/hadoop/bin:/opt/hadoop/sbin:$PATH
    create: yes
  tags:
    - hadoop

- name: edit /home/hadoop/.bashrc
  blockinfile:
    dest: /home/hadoop/.bashrc
    block: |
      export HADOOP_HOME=/opt/hadoop/
      export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
  tags:
    - hadoop

- name: edit /root/.bashrc
  blockinfile:
    dest: /root/.bashrc
    block: |
      export HADOOP_HOME=/opt/hadoop/
      export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
  tags:
    - hadoop

#- name: creare ssh key and copy the public key to the data nodes worker01,worker02
#Till this time : i am creating the key manually , and sharing the oublic key with the data nodes
#

- name: Configure Hadoop core-site.xml file
  blockinfile:
    path: /opt/hadoop/etc/hadoop/core-site.xml 
    insertafter: "<configuration>"
    block: |
      <property>
      <name>fs.default.name</name>
      <value>hdfs://hdfs-master.lab.example.com:9000/</value>
      </property>
      <property>
      <name>dfs.permissions</name>
      <value>false</value>
      </property>
  tags: 
    - hadoop

- name: Configure Hadoop hdfs-site.xml 
  blockinfile:
    path: /opt/hadoop/etc/hadoop/hdfs-site.xml
    insertafter: "<configuration>"
    block: |
      <property>
      <name>dfs.namenode.name.dir</name>
      <value>/opt/hadoop/data/nameNode</value>
      </property>
      <property>
      <name>dfs.datanode.data.dir</name>
      <value>/opt/hadoop/data/dataNode</value>
      </property>
      <property>
      <name>dfs.replication</name>
      <value>1</value>
      </property>
  tags:
    - hadoop

- name: Configure Hadoop mapred-site.xml
  blockinfile:
    path: /opt/hadoop/etc/hadoop/mapred-site.xml
    insertafter: "<configuration>"
    block: |
      <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
      </property>
      <property>
      <name>yarn.app.mapreduce.am.env</name>
      <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
      </property>
      <property>
      <name>mapreduce.map.env</name>
      <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
      </property>
      <property>
      <name>mapreduce.reduce.env</name>
      <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
      </property>
      <property>
      <name>yarn.app.mapreduce.am.resource.mb</name>
      <value>512</value>
      </property>
      <property>
      <name>mapreduce.map.memory.mb</name>
      <value>256</value>
      </property>
      <property>
      <name>mapreduce.reduce.memory.mb</name>
      <value>256</value>
      </property>
  tags:
    - hadoop

- name: Configure Hadoop yarn-site.xml
  blockinfile:
    path: /opt/hadoop/etc/hadoop/yarn-site.xml
    insertafter: "<configuration>"
    block: |
     <property>
     <name>yarn.acl.enable</name>
     <value>0</value>
     </property>
     <property>
     <name>yarn.resourcemanager.hostname</name>
     <value>hdfs-master.lab.example.com</value>
     </property>
     <property>
     <name>yarn.nodemanager.aux-services</name>
     <value>mapreduce_shuffle</value>
     </property>
     <property>
     <name>yarn.nodemanager.resource.memory-mb</name>                                                                  <value>1536</value>
     </property>
     <property>
     <name>yarn.scheduler.maximum-allocation-mb</name>
     <value>1536</value>
     </property>
     <property>
     <name>yarn.scheduler.minimum-allocation-mb</name>
     <value>128</value>
     </property>
     <property>
     <name>yarn.nodemanager.vmem-check-enabled</name>
     <value>false</value>
     </property>
  tags:
    - hadoop

- name: Configure Hadoop env.sh file
  blockinfile:
    path: /opt/hadoop/etc/hadoop/hadoop-env.sh
    insertafter: "<configuration>"
    block: |
      export JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk
      export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true
      export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
  tags:
    - hadoop

- name: configure worker nodes , remove localhost
  lineinfile:
    dest: /opt/hadoop/etc/hadoop/workers
    regexp: "localhost"
    state: absent
  tags:
    - hadoop

- name: configure worker nodes , add worker nodes - datanodes
  blockinfile:
    path: /opt/hadoop/etc/hadoop/workers
    block: |
      hdfs-worker02.lab.example.com
      hdfs-worker01.lab.example.com
  tags:
    - hadoop
